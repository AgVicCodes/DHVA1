{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgVicCodes/DHVA1/blob/main/tutorial_4_AutoML_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial week 11: Auto ML, and  networks**\n",
        "\n",
        "This tutorial allows you to extend the end-to-end case study we did in the [tutorial from week 3], by including the following additional steps:\n",
        "\n",
        "* Step 1: **Data imputation**\n",
        "\n",
        "* Step 2: **Auto ML model comparison**\n",
        "\n",
        "* Step 3: **Networks**\n",
        "\n",
        "It is modified from a Colab Notebook originally written by Rob Yates. Follow the instructions provided below, modifying/adding code where necessary to complete each step. You can use code snippets presented in Lecture 4 where necessary."
      ],
      "metadata": {
        "id": "PeAnekmB-GNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "wB3xm7pm6kS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 0: Loading the data**\n",
        "\n",
        "First, mount your Google Drive and load the heart.csv file into a Pandas DataFrame, as we did in [tutorial from week 10](https://colab.research.google.com/drive/1fnDL9lYMdbuP8XnIFVMsMZKmvi4zdvxY?usp=sharing). Then, execute the subseqent cells to encode the non-numerical features."
      ],
      "metadata": {
        "id": "c-fEeXPiJw8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBZAjOqhq0Hl"
      },
      "outputs": [],
      "source": [
        "#[Write code to mount your Google Drive and load the Heart Disease data set into a Pandas DataFrame called \"df\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encoding\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "label_columns = ['Sex', 'ExerciseAngina']\n",
        "\n",
        "for label in label_columns:\n",
        "    df[label] = le.fit_transform(df[label])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "syxym3hRuMBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "\n",
        "df = pd.get_dummies(df, columns = ['ChestPainType', 'RestingECG', 'ST_Slope'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "x1vMQc1BuHMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Data imputation**\n",
        "\n",
        "As discussed in the third tutorial and in Lecture 4, the 'Cholesterol' feature in this dataset has a large number of 0 values. These are likely to be \"missing data\". Check this by plotting the historgram below, then implement the **MICE imputation technique** discussed in Lectures 2 and 4 to replace these 0s with realistic values.\n",
        "\n",
        "N.B. MICE imputation works by replacing NaNs, so you'll need to convert the 0s to NaNs first.\n"
      ],
      "metadata": {
        "id": "3lid4PxNLAvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot of 'Cholesterol' feature before imputation\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(x='Cholesterol', data=df, hue='HeartDisease')\n",
        "plt.title('Cholesterol distribution before data imputation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "njxlaI59t-ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [In this cell, use the information in Lecture 2 and Lecture 4 on MICE imputation to replace the 0s in the 'Cholesterol' feature with realistic values]\n",
        "# [Remember to replace the 0s in the 'Cholesterol' feature with NaNs first.]\n"
      ],
      "metadata": {
        "id": "Xq84H1vTGIXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot of 'Cholesterol' feature after imputation\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(x='Cholesterol', data=df_imputed, hue='HeartDisease')\n",
        "plt.title('Cholesterol distribution after data imputation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i8bDXIfZKnFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Auto ML**\n",
        "\n",
        "Now that the data is pre-processed, we can feed it into ML models. We could do this manually, but it can be cumbersome and inefficient to train & compare many models this way.\n",
        "\n",
        "Instead, **Auto ML packages allow many ML models to be efficiently trained & compared to each other.** They also provide insight int the dataset through XAI techniques.\n",
        "\n",
        "Below, we will apply the **H20AutoML** package to the heart disease dataset.\n"
      ],
      "metadata": {
        "id": "KOjw3AxeLEjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install H20AutoML (only need to do this once)\n",
        "!pip install h2o\n",
        "\n",
        "# Import packages\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Start the H2O cluster (locally)\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "Jek5v5X5NOza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target variable (i.e. \"response\") and features (i.e. \"predictors\") for H20AutoML\n",
        "response = 'HeartDisease'\n",
        "predictors = list(df_imputed.drop('HeartDisease', axis=1).columns)\n",
        "\n",
        "# Convert from a pandas dataframe to an h20 frame\n",
        "hf = h2o.H2OFrame(df_imputed)\n",
        "\n",
        "# Convert target column to categorical, as this is a binary classification problem\n",
        "hf['HeartDisease'] = hf['HeartDisease'].asfactor()\n",
        "\n",
        "# Split the data into training and testing sets using the H20 split_frame() method\n",
        "train, test = hf.split_frame(seed=1, ratios=[0.75])"
      ],
      "metadata": {
        "id": "kmk-Th5YurIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below sets-up and trains a series of different ML models on our dataset. Before running the cell, use the [H20AutoML online documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) to answer the following questions:\n",
        "\n",
        "*   **Below, we have chosen three ML algorithms (\"GLM\", \"DRF\", \"XGBoost\"). What do these acronyms stand for?**\n",
        "*   **We have also chosen to limit the training time to 120 secs, via the \"max_runtime_sec\" argument. What other stopping parameter options are available in H20AutoML?**"
      ],
      "metadata": {
        "id": "TST9FWU1TlV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set-up and train the models\n",
        "aml = H2OAutoML(max_runtime_secs=120, seed=1, include_algos=[\"GLM\", \"DRF\", \"XGBoost\"])\n",
        "aml.train(x=predictors, y=response, training_frame=train)\n",
        "\n",
        "# Turn off default printed output\n",
        "h2o.display.toggle_user_tips()"
      ],
      "metadata": {
        "id": "fHZu--TxOU6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below plots a \"leaderboard\" of the top 20 best-performing models for our dataset, including a number of different performance metrics. Run the cell and then answer the following questions:\n",
        "\n",
        "*   **Which of the three model types we considered above (\"GLM\", \"DRF\", \"XGBoost\") perform best overall?**\n",
        "*   **For binary classification tasks, H20AutoML ranks  models  by their \"AUC\" score. What is an \"AUC\" score?**\n",
        "\n"
      ],
      "metadata": {
        "id": "sSC99qVSSed3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show leaderboard & performance information\n",
        "aml.explain(test, include_explanations=[\"leaderboard\"]);"
      ],
      "metadata": {
        "id": "h2MuPmB7Sv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below plots three different XAI methods for the various models  trained by H20AutoML. Run the cell, and then answer the following questions:\n",
        "\n",
        "* **Explain the main results that these  XAI plots provide**.\n",
        "* Use the [H20AutoML online documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) to **plot other XAI methods for a wider range of attributes**. Can you discover any other interesting trends?\n",
        "\n"
      ],
      "metadata": {
        "id": "Om47swv0Uw0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot XAI information\n",
        "aml.explain(test, include_explanations=[\"varimp_heatmap\", \"shap_summary\", \"pdp\"], \\\n",
        "            columns = [\"ST_Slope_Up\", \"ChestPainType_ASY\", \"Cholesterol\"]);"
      ],
      "metadata": {
        "id": "dwVXJMV9OfcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Networks**\n",
        "\n",
        "Finally, we will explore the similarities between the different instances in our dataset by creating a tree graph using the **NetworkX** package. Complete the cells below, then answer the following questions:\n",
        "\n",
        "*  What does the **\"minimum spanning tree (MST)\"** created below tell us about our dataset? Are patients with/without heart disease well separated? Are there sub-groups containing mostly patients with or without heart disease?\n",
        "* Use the [Network X online documentation](https://networkx.org/documentation/stable/) to try creating **different types of tree and tree algorithm**. Do they return very different results?"
      ],
      "metadata": {
        "id": "c9fVKLyPVpvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NetworkX (only need to do once)\n",
        "!pip install networkx\n",
        "\n",
        "# Import  package\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "3aI50mioVsWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "df_imputed[\"ID\"] = df_imputed.index\n",
        "\n",
        "# Compute Euclidean distances between all feature values\n",
        "distance_vector = pdist(df_imputed.drop(columns=[\"HeartDisease\", \"ID\"]), metric=\"cityblock\")\n",
        "\n",
        "# Compute square distance matrix\n",
        "distance_matrix = squareform(distance_vector)\n",
        "\n",
        "# Convert to Network X graph\n",
        "undirected_graph = nx.from_numpy_array(distance_matrix)"
      ],
      "metadata": {
        "id": "aQt07Ot6V6dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create minimum spanning tree (MST)\n",
        "mst_graph = nx.minimum_spanning_tree(undirected_graph, algorithm=\"prim\")"
      ],
      "metadata": {
        "id": "CSS0HvC8Xn6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set-up info for plot\n",
        "node_list = list(mst_graph.nodes())\n",
        "node_color = df.loc[node_list, \"HeartDisease\"].map({0: \"red\", 1: \"green\"})\n",
        "pos = nx.kamada_kawai_layout(mst_graph)"
      ],
      "metadata": {
        "id": "nxSQWDslX3iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "nx.draw_networkx(mst_graph, pos, node_color=node_color, node_size=100, with_labels=False, edge_color=\"grey\")\n",
        "plt.title(\"Heart Disease Minimum Spanning Tree\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AewRQRknX5mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}